{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95072fc-6e36-4999-8720-02e055abe583",
   "metadata": {},
   "source": [
    "# ML/XAI Tool Chest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add9fe6f-3908-40b4-9bc3-7a2bfb763b1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f27eb03c-9b83-4353-a30a-ed1301808332",
   "metadata": {},
   "source": [
    "from package_installer_updater import install_or_update_packages_from_code\n",
    "\n",
    "code = \"\"\"\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import numpy as np\n",
    "import pprint\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import time\n",
    "import pytz\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "import pickle\n",
    "import sklearn.model_selection as model\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "import xgboost as xgb\n",
    "# hyper-parameter tuning\n",
    "from scipy.stats import randint as sp_randint\n",
    "import sys\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler, NearMiss, TomekLinks\n",
    "from imblearn.combine import SMOTETomek\n",
    "\"\"\"\n",
    "\n",
    "install_or_update_packages_from_code(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "114008c1-3af1-4b73-8e52-6ecf0653c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import numpy as np\n",
    "import pprint\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import time\n",
    "import pytz\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "#import pickle\n",
    "import sklearn.model_selection as model\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "import xgboost as xgb\n",
    "# hyper-parameter tuning\n",
    "from scipy.stats import randint as sp_randint\n",
    "import sys\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler, NearMiss, TomekLinks\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f885ae-6a79-4dee-97d7-d65a0b2c6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "780c9db0-7aff-4216-b8c3-9b15bc2aef99",
   "metadata": {},
   "source": [
    "# neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64a1f226-e525-4b9a-9d35-f1572361e1e8",
   "metadata": {},
   "source": [
    "# model interpretability\n",
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582bdd87-eb8e-465b-bdd0-4801362e1462",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1) Data Loading & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb140602-42a3-4148-9c6c-9f95461d457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and split into train and test sets\n",
    "def split_data(df, target_var, total_record_cnt=-1, train_size=0.7, test_size=0.3, valid_size=0, rand_state=42):\n",
    "\n",
    "    # error catch - validate input parameters    \n",
    "    if (train_size + test_size + valid_size) != 1:\n",
    "        raise ValueError(\"Selected sample sizes don't add up to 1! Please correct.\")\n",
    "\n",
    "    # random sample (if selected)\n",
    "    if total_record_cnt>0:\n",
    "        df = df.sample(n=size, replace=False, random_state=rand_state)\n",
    "    \n",
    "    # X and y splitting\n",
    "    y = df[target_var]\n",
    "    X = df.drop(target_var, axis=1)\n",
    "\n",
    "    if valid_size>0:\n",
    "        # train-test-valid splitting\n",
    "        X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=train_size)\n",
    "        X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=(test_size/(test_size+valid_size)), random_state=42)\n",
    "\n",
    "        # User message\n",
    "        print('\\n\\033[1mTrain-Test-Valid Splitting Results:\\033[0m\\n')\n",
    "        print(f'Training: \\t{100*train_size}%, \\t{X_train.shape[0]} samples.')\n",
    "        print(f'Test: \\t\\t{100*test_size}%,  {X_test.shape[0]} samples.')\n",
    "        print(f'Validation: \\t{100*valid_size}%,  \\t{X_valid.shape[0]} samples.\\n')\n",
    "        \n",
    "        return X_train, X_test, X_valid, y_train, y_test, y_valid\n",
    "\n",
    "    else:\n",
    "        # train-test splitting\n",
    "        X_train, X_test, y_train, y_test = model.train_test_split(X, y, train_size=train_size, random_state=42)\n",
    "\n",
    "        # User message\n",
    "        print('\\n\\033[1mTrain-Test Splitting Results:\\033[0m\\n')\n",
    "        print(f'Training: \\t{100*train_size}%, \\t{X_train.shape[0]} samples.')\n",
    "        print(f'Test: \\t\\t{100*test_size}%,  {X_test.shape[0]} samples.\\n')\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c4e46d-b4e9-47b1-a3de-079a496a721b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2) Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2394057-0d36-4218-a74e-2097f95e4c7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## a) Class Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5501688-70ec-4e64-921c-b356d1166761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply over/under-sampling methods for class balancing\n",
    "def class_balancer(X, y, method, sampling_strategy=1, version=1, n_neighbors=3):\n",
    "\n",
    "    # error catch - validate input parameters\n",
    "    if sampling_strategy not in ['SMOTE', 'ClusterCentroids', 'RandomUnder', 'NearMiss', 'Tomek', 'SMOTETomek']:\n",
    "        raise ValueError(\"Please select a sampling strategy! Choose between 'SMOTE', 'ClusterCentroids', 'RandomUnder', 'NearMiss', 'Tomek', or 'SMOTETomek'.\")\n",
    "    \n",
    "    ### OVER-SAMPLING ###\n",
    "    # SMOTE\n",
    "    if method==\"SMOTE\":    \n",
    "        sampler = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    \n",
    "    ### UNDER-SAMPLING ###\n",
    "    # ClusterCentroid\n",
    "    elif method==\"ClusterCentroids\":\n",
    "        sampler = ClusterCentroids(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    # RandomUnderSampler\n",
    "    elif method==\"RandomUnder\":\n",
    "        sampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    # NearMiss\n",
    "    elif method==\"NearMiss\":\n",
    "        sampler = NearMiss(sampling_strategy=sampling_strategy, version=version, n_neighbors=n_neighbors)\n",
    "    # TomekLinks\n",
    "    elif method==\"Tomek\":\n",
    "        sampler = TomekLinks(sampling_strategy=sampling_strategy)\n",
    "\n",
    "    ### HYBRID ###\n",
    "    # SMOTE-Tomek\n",
    "    elif method==\"SMOTETomek\":\n",
    "        sampler = SMOTETomek(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    \n",
    "    # apply sampling method\n",
    "    X, y = sampler.fit_resample(X, y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeb62bf-346e-4e61-b068-f637099ec4df",
   "metadata": {},
   "source": [
    "## b) Feature Scaling (Standardization & Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e3ae39-8cfc-4cef-a999-42d0e242b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply normalization or standardization scaling and return scaled data as well as scaler\n",
    "def scaler_fit_and_transform(df, cols, scaler):\n",
    "\n",
    "    # error catch - validate input parameters\n",
    "    if scaler not in ['MinMax', 'Standard']:\n",
    "        raise ValueError(\"Please select a valid scaler! Choose between 'MinMax' or 'Standard'.\")\n",
    "    \n",
    "    # create copy of input data\n",
    "    out = df.copy()\n",
    "\n",
    "    # initialize scaler\n",
    "    if scaler == 'MinMax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaler == 'Standard':\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    # fit scaler and transform data\n",
    "    out[cols] = scaler.fit_transform(out[cols])\n",
    "    #out = pd.DataFrame(out, columns=df.columns)\n",
    "\n",
    "    # User message\n",
    "    print('\\n\\033[1mFeature Scaling Results - Training Data:\\033[0m\\n')\n",
    "    print(f'The following numerical features were scaled using the \"{scaler}\":\\n')\n",
    "    print('\"'+', '.join(cols)+'\"\\n')\n",
    "    display(out.describe())\n",
    "        \n",
    "    return out, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfc5cb12-2d75-43d7-8c0c-362dbc1f21c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use already fitted scaler to e.g. scale test set or new observations\n",
    "def scaler_transform(df, cols, scaler_fitted):\n",
    "\n",
    "    # create copy of input data\n",
    "    out = df.copy()\n",
    "    \n",
    "    # transform data using fitted scaler\n",
    "    out[cols] = scaler_fitted.transform(out[cols])\n",
    "    #out = pd.DataFrame(out, columns=df.columns)\n",
    "\n",
    "    # User message\n",
    "    print('\\n\\033[1mFeature Scaling Results - Test/Validation Data:\\033[0m\\n')\n",
    "    print(f'The following numerical features were scaled using the pre-fitted \"{scaler}\":\\n')\n",
    "    print('\"'+', '.join(cols)+'\"\\n')\n",
    "    display(out.describe())\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded31270-3aac-4964-8824-f0a42d1d4fef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## c) Train/Test-Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e2a56b-49a3-40e4-a1dd-d3b1a91c5a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the respective datasets based on selected scaling and sampling method as well as sampling size \n",
    "def select_X_y(X_train, y_train, X_test, y_test, sampler=None, scaler=None, total_record_cnt=-1,\n",
    "               sampling_strategy=1, version=0, n_neighbors=0):\n",
    "\n",
    "    # apply class balancer function\n",
    "    if sampler:\n",
    "        X_train, y_train = class_balancer(X_train, y_train, sampler, sampling_strategy, version, n_neighbors)\n",
    "\n",
    "    # apply scaler function\n",
    "    if scaler:\n",
    "        X_train, scaler_fitted = scaler_fit_transform(X_train, cols_to_scale, scaler)\n",
    "        X_test = scaler_transform(X_test, cols_to_scale, scaler_fitted)\n",
    "            \n",
    "    # reduce train-sample size\n",
    "    if total_record_cnt>0:\n",
    "        X_train = X_train.sample(n=size, replace=False, random_state=42)\n",
    "        y_train = y_train[y_train.index]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a8d46a-6a30-4f91-be13-cc83eddf7811",
   "metadata": {},
   "source": [
    "## d) One-Hot-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9d29297-e363-4b84-82bd-083d5c1d46d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply one-hot encoding on categorical features\n",
    "def one_hot_encoding(df, cols, drop_first=True, dtype=bool, info=True):\n",
    "\n",
    "    # create copy of input dataframe\n",
    "    out = df.copy()\n",
    "    \n",
    "    for col in cols:\n",
    "        out = pd.get_dummies(data=out, columns=[col], drop_first=drop_first, prefix=col, dtype=dtype)\n",
    "\n",
    "    if info:\n",
    "        # User message\n",
    "        print('\\n\\033[1mOne-Hot Encoding Results:\\033[0m\\n')\n",
    "        print(f'The following categorical features were encoded:\\n')\n",
    "        print('\"'+', '.join(cols)+'\"\\n')\n",
    "        display(out.head(3))\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c97b279-24b4-44da-9819-1568b8844d0a",
   "metadata": {},
   "source": [
    "## e) Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b05faa61-51b1-40b0-8408-de22a7bd531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_removal(X, y, cols, detection_method='iqr', thres=1.5, handling_method='cap'):\n",
    "\n",
    "    # error catch - validate input parameters\n",
    "    if detection_method not in ['iqr', 'std', 'zscore']:\n",
    "        raise ValueError(\"Please select a valid detection method! Choose betwen 'iqr', 'std', or 'zscore'.\")\n",
    "\n",
    "    if handling_method not in ['drop', 'cap']:\n",
    "        raise ValueError(\"Please select a valid handling method! Choose between 'drop' or 'cap'.\")\n",
    "    \n",
    "    # create copy of input dataframe\n",
    "    X_temp = X.copy()\n",
    "    y_temp = y.copy()\n",
    "\n",
    "    # List to store outlier indices across all columns\n",
    "    ind_all_outliers = []\n",
    "\n",
    "    # looping through passed on columns\n",
    "    for col in cols:\n",
    "\n",
    "        # outlier detection\n",
    "        ## IQR method\n",
    "        if detection_method == 'iqr':\n",
    "            q1 = X_temp[col].quantile(0.25)\n",
    "            q3 = X_temp[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            upper = q3 + (thres * iqr)\n",
    "            lower = q1 - (thres * iqr)\n",
    "            ind_col_outlier = X_temp[(X_temp[col] > upper) | (X_temp[col] < lower)].index\n",
    "      \n",
    "        ## standard-deviation method\n",
    "        elif detection_method == 'std':\n",
    "            mean = np.mean(X_temp[col])\n",
    "            std = np.std(X_temp[col])\n",
    "            upper = mean + (thres * std)\n",
    "            lower = mean - (thres * std)\n",
    "            ind_col_outlier = X_temp[(X_temp[col] > upper) | (X_temp[col] < lower)].index\n",
    "            \n",
    "        ## z-score method\n",
    "        elif detection_method == 'zscore':\n",
    "            mean = np.mean(X_temp[col])\n",
    "            std = np.std(X_temp[col])\n",
    "            ind_col_outlier = X_temp[abs((X_temp[col] - mean) / std) > thres].index\n",
    "\n",
    "        \n",
    "        # outlier handling\n",
    "        ## dropping outliers\n",
    "        if handling_method == 'drop':\n",
    "            X_temp = X_temp.drop(ind_col_outlier)\n",
    "            y_temp = y_temp.drop(ind_col_outlier)\n",
    "        ## capping outliers to max value\n",
    "        elif handling_method == 'cap':\n",
    "            X_temp.loc[ind_col_outlier, col] = upper\n",
    "\n",
    "        # Add the indices of outliers for the current column to the overall list\n",
    "        ind_all_outliers.extend(ind_col_outlier)\n",
    "\n",
    "    # User message\n",
    "    print('\\n\\033[1mOutlier Removal Results:\\033[0m\\n')\n",
    "    print(f'Detected {len(ind_all_outliers)} outliers using the \"{detection_method}\" approach.')\n",
    "    print(f'Outliers have been \"{handling_method}ped\".\\n')\n",
    "    print(f'The dataset now has {X_temp.shape[0]} samples.\\n')\n",
    "\n",
    "    return X_temp, y_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64458057-a5d1-4cc5-b651-515a3bc5299a",
   "metadata": {},
   "source": [
    "## f) Missing Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7306f2b7-af31-472a-9a7f-4851d59c6ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_data(df):\n",
    "\n",
    "    # Count of missing values per column\n",
    "    missing_count = df.isnull().sum()\n",
    "\n",
    "    # Percentage of missing values per column\n",
    "    missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "    # Combine count and percentage into a single DataFrame for a better display\n",
    "    print('\\n\\033[1mMissing Data Analysis Results:\\033[0m\\n')\n",
    "    missing_data = pd.DataFrame({'Missing Count': missing_count, 'Percentage (%)': missing_percentage})\n",
    "\n",
    "    return missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bbc104-eac4-417e-8c60-f9568c5a32f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3) Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad10bc4-cfa2-4515-b79f-19dba5b99060",
   "metadata": {
    "tags": []
   },
   "source": [
    "## a) Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a16be-e545-4feb-8ee2-48598db1d77b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### i) \"White Box\" Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3b55d51-d5ed-4a54-b017-acef5173f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit classifier/regressor\n",
    "def fit_model(X, y, algo):\n",
    "    \n",
    "    # create copy of input classifier\n",
    "    model = algo\n",
    "    \n",
    "    # fit the model\n",
    "    model = model.fit(X, y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b2450e-d324-475b-9b8f-66cbaad01bc5",
   "metadata": {},
   "source": [
    "#### ii) Neural Network (\"Black Box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc8eff1d-8050-4648-a2de-6207ffc0c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning evolution plot\n",
    "def plot_learning_evolution(clf):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(clf.history['loss'], label='Loss')\n",
    "    plt.plot(clf.history['val_loss'], label='val_Loss')\n",
    "    plt.title('Loss evolution during trainig')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(clf.history['AUC'], label='AUC')\n",
    "    plt.plot(clf.history['val_AUC'], label='val_AUC')\n",
    "    plt.title('AUC score evolution during trainig')\n",
    "    plt.legend()\n",
    "\n",
    "# fit neural network\n",
    "def fit_nn(X_train, y_train, X_test, y_test, num_cols, num_labels, hidden_units, dropout_rates, \n",
    "           learn_rate, epochs=20, batch_size=32):\n",
    "    \n",
    "    inp = tf.keras.layers.Input(shape=(num_cols))\n",
    "    x = BatchNormalization()(inp)\n",
    "    x = Dropout(dropout_rates[0])(x)\n",
    "    for i in range(len(hidden_units)):\n",
    "        x = Dense(hidden_units[i], activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(dropout_rates[i + 1])(x)\n",
    "    x = Dense(num_labels, activation='sigmoid')(x)\n",
    "  \n",
    "    clf = Model(inputs=inp, outputs=x)\n",
    "    clf.compile(optimizer=Adam(learn_rate), loss='binary_crossentropy', metrics=[AUC(name='AUC')])\n",
    "    \n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size)\n",
    "    \n",
    "    #plot_learning_evolution(clf)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e271169-8e4b-4f46-9a8d-5c8ddc92170e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## b) Model Deployment / Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b9e4ea6-3cb3-440b-a4f1-a7ea61d2e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions using fitted classifier\n",
    "def predict_clf(X, clf, thres=0.5):\n",
    "    \n",
    "    # make predcitions\n",
    "    y_prob = clf.predict_proba(X)[:,1]\n",
    "    y_pred = [1 if x >= thres else 0 for x in y_prob]\n",
    "    \n",
    "    return y_pred, y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e4370-c741-4e4b-a86b-2fc0d552f848",
   "metadata": {
    "tags": []
   },
   "source": [
    "## c) Parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0ef49-4263-4e05-a190-b48611011398",
   "metadata": {
    "tags": []
   },
   "source": [
    "### i) Optimal Classifier Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7f8c8fa-8d15-461d-af91-15fcbfc4b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return summary table that shows performance metrics across different classifier threshold levels\n",
    "def grid_search_thres(clf, X_test, y_test, step=0.01, show=False):\n",
    "    \n",
    "    # initialize output table\n",
    "    thres_grid = pd.DataFrame(index=['Accuracy', 'Balanced Acc', 'Precision', 'Recall', 'F1','AUC'])\n",
    "\n",
    "    # stepwise loop through threshold levels\n",
    "    for i in np.arange(0, 1.0, step):\n",
    "        # assign target labels based on current threshold\n",
    "        y_pred, y_prob = predict(X_test, clf, thres=i)\n",
    "        \n",
    "        # add metrics to output table\n",
    "        thres_grid[i] = [round(metrics.accuracy_score(y_test, y_pred),4),\n",
    "                         round(metrics.balanced_accuracy_score(y_test, y_pred),4),\n",
    "                         round(metrics.precision_score(y_test, y_pred),4),\n",
    "                         round(metrics.recall_score(y_test, y_pred),4),\n",
    "                         round(metrics.f1_score(y_test, y_pred, average='weighted'),4),\n",
    "                         round(metrics.roc_auc_score(y_test, y_prob),4)]\n",
    "        \n",
    "    if show: display(thres_grid)\n",
    "            \n",
    "    return thres_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15db8587-030b-4ebe-8966-581dd659482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the optimal threshold level using Youden's J statistic or Precision-Recall Curve approach\n",
    "def get_opt_thres(clf, X_test, y_test):\n",
    "    \n",
    "    y_prob = clf.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Youdenâ€™s J statistic\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_prob)\n",
    "    score = tpr - fpr\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    \"\"\"\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_prob)\n",
    "    score = (2 * precision * recall) / (precision + recall)\n",
    "    \"\"\"\n",
    "    \n",
    "    i = np.argmax(score)\n",
    "    opt_thres = thresholds[i]\n",
    "    print('Optimal Threshold: %.4f' % (opt_thres))\n",
    "    \n",
    "    return opt_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49c813e3-e3e5-4194-b11f-9ee68490bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all evaluation metrics with moving threshold level\n",
    "def plot_thres(clf, X_test, y_test, step=0.01, metric=None):\n",
    "            \n",
    "    # threshold grid search table\n",
    "    thres_grid = grid_search_thres(clf, X_test, y_test, step)\n",
    "\n",
    "    # get optimal threshold\n",
    "    opt_thres = get_opt_thres(clf, X_test, y_test)\n",
    "    \n",
    "    # plot curve for selected metric\n",
    "    if metric != None:\n",
    "        temp = pd.DataFrame(thres_grid.loc[metric]).reset_index()\n",
    "        temp.rename(columns = {'index':'Threshold'}, inplace = True)\n",
    "        plt.ylim(0, 1)\n",
    "        sns.lineplot(data=temp, x='Threshold', y=metric).set(title='Threshold Tuning Curve for '+metric)\n",
    "        plt.axvline(opt_thres, color='r')\n",
    "        plt.text(opt_thres+0.03, 0.9,'t=%.2f' % opt_thres, color='r')\n",
    "        \n",
    "    # plot curves for all metrics\n",
    "    else:\n",
    "        i = 1\n",
    "        m_list = list(thres_grid.index.values)\n",
    "        del m_list[-1]\n",
    "        for m in m_list:\n",
    "            temp = pd.DataFrame(thres_grid.loc[m]).reset_index()\n",
    "            temp.rename(columns = {'index':'Threshold'}, inplace = True)\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            plt.subplot(3, 2, i)\n",
    "            plt.ylim(0, 1)\n",
    "            sns.lineplot(data=temp, x='Threshold', y=m).set(title='Threshold Tuning Curve for '+m)\n",
    "            plt.axvline(opt_thres, color='r')\n",
    "            plt.text(opt_thres+0.03, 0.9,'t=%.2f' % opt_thres, color='r')\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891be54d-2783-4ef0-a129-5cd0050b47e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ii) Hyper-Parameters: Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2101d7eb-63a0-49be-b581-f7ed7a30b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter tuning using random search\n",
    "def random_search(X, y, clf, params, n_iter=5, cv=3, k_fold=3, scoring='balanced_accuracy'):\n",
    "    \n",
    "    # shuffle splitting\n",
    "    cv = model.ShuffleSplit(n_splits=k_fold, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # define random search parameters\n",
    "    randomCV = model.RandomizedSearchCV(clf, param_distributions=params, n_iter=n_iter, cv=cv, scoring=scoring, random_state=42)\n",
    "    \n",
    "    # fit models\n",
    "    randomCV.fit(X, y)\n",
    "    \n",
    "    return randomCV.best_estimator_, randomCV.best_params_, randomCV.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93740955-11e8-4292-b970-674722f9fc94",
   "metadata": {
    "tags": []
   },
   "source": [
    "### iii) Hyper-Parameters: Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a18d0cd-832e-421b-ad71-909ab0d76455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-parameter tuning using bayesian optimization\n",
    "def bayesian_opt():\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2939ca3a-7675-452a-a147-32c1b480e0a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### iv) Optimal Dataset Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60828eaa-2892-449c-a31b-a9f8b56cb687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal sampling and scaling method for specific classifier using specified eval metric\n",
    "def get_optimal_data(df, clf, metric='AUC'):\n",
    "    \n",
    "    # find index for optimal score for selected clf\n",
    "    idx = df.loc[df['Clf']==clf][[metric]].idxmax()\n",
    "    \n",
    "    # extract optimal parameters\n",
    "    scaler = str(df.loc[idx, 'Scaling'][0])\n",
    "    sampler = str(df.loc[idx, 'Sampling'][0])\n",
    "    \n",
    "    if scaler=='non':\n",
    "        scaler = None\n",
    "    if sampler=='non':\n",
    "        sampler = None\n",
    "    \n",
    "    return scaler, sampler "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b9ca6-b80e-48c1-a052-b939a3f9d00f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## d) Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64e0dc6e-a65c-4563-95f2-4fb892ccc807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate model\n",
    "def cv_model(X, y, clf, clf_name, k_fold=3):\n",
    "    \n",
    "    # shuffle split\n",
    "    cv = model.ShuffleSplit(n_splits=k_fold, test_size=0.3, random_state=42)\n",
    "\n",
    "    # initialize scoring metrics and summary tables\n",
    "    scoring = ['accuracy','balanced_accuracy','precision','recall','f1','roc_auc']\n",
    "    model_eval = pd.DataFrame(index=['Accuracy', 'Bal_Acc', 'Precision', 'Recall', 'F1', 'AUC'])\n",
    "    model_eval_detail = pd.DataFrame(index=['Fold', 'Fit_Time', 'Score_Time', 'Accuracy', 'Bal_Acc', 'Precision', 'Recall', 'F1', 'AUC'])\n",
    "\n",
    "    cv_scores = model.cross_validate(clf, X, y, cv=cv, scoring=scoring, return_estimator=True)\n",
    "\n",
    "    # CV aggegrated scores\n",
    "    model_eval[clf_name] = [round(cv_scores['test_accuracy'].mean(),4),\n",
    "                            round(cv_scores['test_balanced_accuracy'].mean(),4),\n",
    "                            round(cv_scores['test_precision'].mean(),4),\n",
    "                            round(cv_scores['test_recall'].mean(),4),\n",
    "                            round(cv_scores['test_f1'].mean(),4),\n",
    "                            round(cv_scores['test_roc_auc'].mean(),4)]\n",
    "    \n",
    "    # CV scores for each fold\n",
    "    model_eval_detail[clf_name] = [list(range(1,len(cv_scores['test_accuracy'])+1)),\n",
    "                                   np.round(cv_scores['fit_time'],2),\n",
    "                                   np.round(cv_scores['score_time'],2),\n",
    "                                   np.round(cv_scores['test_accuracy'],4),\n",
    "                                   np.round(cv_scores['test_balanced_accuracy'],4),\n",
    "                                   np.round(cv_scores['test_precision'],4),\n",
    "                                   np.round(cv_scores['test_recall'],4),\n",
    "                                   np.round(cv_scores['test_f1'],4),\n",
    "                                   np.round(cv_scores['test_roc_auc'],4)]\n",
    "    \n",
    "    #display(model_eval)\n",
    "    #display(model_eval_detail)\n",
    "\n",
    "    return model_eval, model_eval_detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15532ac-8100-41b7-b54f-bda88e3420ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## e) Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c42e4f-74cd-494e-9f05-d31824beaf91",
   "metadata": {},
   "source": [
    "### (i) Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13b559bb-f1b0-4260-81bb-f24cdd0e3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate (final) model\n",
    "def eval_model(X_test, y_test, X_train, y_train, clf_fitted, clf_name, thres=0.5, show=True):\n",
    "    \n",
    "    # initialize scoring metrics and summary table\n",
    "    scoring = ['accuracy','balanced_accuracy','precision','recall','f1','roc_auc']\n",
    "    model_eval = pd.DataFrame(index=['RunTime', 'Accuracy', 'Bal_Acc', 'Precision', 'Recall', 'F1', 'AUC'])\n",
    "    \n",
    "    # get preds and probs\n",
    "    start_time = time.time()\n",
    "    y_pred_train, y_prob_train = predict(X_train, clf_fitted, thres)\n",
    "    train_time = time.time()\n",
    "    y_pred_test, y_prob_test = predict(X_test, clf_fitted, thres)\n",
    "    test_time = time.time()\n",
    "    \n",
    "    # model scores\n",
    "    model_eval[clf_name+'_train'] = [str(timedelta(seconds=train_time-start_time)),\n",
    "                                     round(metrics.accuracy_score(y_train, y_pred_train),4),\n",
    "                                     round(metrics.balanced_accuracy_score(y_train, y_pred_train),4),\n",
    "                                     round(metrics.precision_score(y_train, y_pred_train),4),\n",
    "                                     round(metrics.recall_score(y_train, y_pred_train),4),\n",
    "                                     round(metrics.f1_score(y_train, y_pred_train, average='weighted'),4),\n",
    "                                     round(metrics.roc_auc_score(y_train, y_prob_train),4)]\n",
    "    \n",
    "    model_eval[clf_name+'_test'] = [str(timedelta(seconds=test_time-train_time)),\n",
    "                                    round(metrics.accuracy_score(y_test, y_pred_test),4),\n",
    "                                    round(metrics.balanced_accuracy_score(y_test, y_pred_test),4),\n",
    "                                    round(metrics.precision_score(y_test, y_pred_test),4),\n",
    "                                    round(metrics.recall_score(y_test, y_pred_test),4),\n",
    "                                    round(metrics.f1_score(y_test, y_pred_test, average='weighted'),4),\n",
    "                                    round(metrics.roc_auc_score(y_test, y_prob_test),4)]\n",
    "    \n",
    "    \n",
    "    display(model_eval.transpose())\n",
    "    \n",
    "    # confusion matrix and ROC curve\n",
    "    cm = metrics.ConfusionMatrixDisplay.from_estimator(clf_fitted, X_test, y_test, cmap='cividis', display_labels=['Fully-Paid','Default'])\n",
    "    roc = metrics.RocCurveDisplay.from_estimator(clf_fitted, X_test, y_test)\n",
    "    \n",
    "    return model_eval.transpose(), cm, roc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94239f8c-f185-4255-b280-f88d2652e957",
   "metadata": {},
   "source": [
    "### (ii) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b4aed8-a5b3-4b00-82df-1ee87fa2e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reg_model(model_name, y_train, y_test, y_pred_train, y_pred_test, n_decimals=10):\n",
    "\n",
    "    # Evaluate the model\n",
    "    ## R2\n",
    "    train_r2 = round(r2_score(y_train, y_pred_train), n_decimals)\n",
    "    test_r2 = round(r2_score(y_test, y_pred_test), n_decimals)\n",
    "    ## Adj. R2\n",
    "    train_adj_r2 = round(1 - (1 - r2_score(y_train, y_pred_train)) * (X_train.shape[0] - 1) / (X_train.shape[0] - X_train.shape[1] - 1), n_decimals)\n",
    "    test_adj_r2 = round(1 - (1 - r2_score(y_test, y_pred_test)) * (X_test.shape[0] - 1) / (X_test.shape[0] - X_test.shape[1] - 1), n_decimals)\n",
    "    ## RSS\n",
    "    train_rss = round(np.sum(np.square(y_train-y_pred_train)), n_decimals)\n",
    "    test_rss = round(np.sum(np.square(y_test-y_pred_test)), n_decimals)\n",
    "    ## MSE\n",
    "    train_mse = round(mean_squared_error(y_train, y_pred_train), n_decimals)\n",
    "    test_mse = round(mean_squared_error(y_test, y_pred_test), n_decimals)\n",
    "    ## RMSE\n",
    "    train_rmse = round(np.sqrt(mean_squared_error(y_train, y_pred_train)), n_decimals)\n",
    "    test_rmse = round(np.sqrt(mean_squared_error(y_test, y_pred_test)), n_decimals)\n",
    "    ## MAE\n",
    "    train_mae = round(mean_absolute_error(y_train, y_pred_train), n_decimals)\n",
    "    test_mae = round(mean_absolute_error(y_test, y_pred_test), n_decimals)\n",
    "    ## MAPE\n",
    "    train_mape = round(np.mean(np.abs((y_train - y_pred_train) / y_train)) * 100, n_decimals)\n",
    "    test_mape = round(np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100, n_decimals)\n",
    "\n",
    "    # Store the results\n",
    "    results = {'Model_Name': model_name,\n",
    "               'Train - R2': train_r2,\n",
    "               'Test - R2': test_r2,\n",
    "               'Train - Adj-R2': train_adj_r2,\n",
    "               'Test - Adj-R2': test_adj_r2,\n",
    "               #'Train - RSS': train_rss,\n",
    "               #'Test - RSS': test_rss,\n",
    "               #'Train - MSE': train_mse,\n",
    "               #'Test - MSE': test_mse,\n",
    "               'Train - RMSE': train_rmse,\n",
    "               'Test - RMSE': test_rmse,\n",
    "               'Train - MAE': train_mae,\n",
    "               'Test - MAE': test_mae,\n",
    "               'Train - MAPE': train_mape,\n",
    "               'Test - MAPE': test_mape}\n",
    "    \n",
    "    return pd.DataFrame([results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb812aae-2e6c-4c21-9e4b-471f44f9bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reg_model_metrics(model_name, y_train, y_test, y_pred_train, y_pred_test):\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 3, figsize=(10, 4))\n",
    "    plt.suptitle(model_name + \" Model\", fontsize=16)\n",
    "    \n",
    "    plt.subplot(2,3,1)\n",
    "    sns.distplot(y_train)\n",
    "    sns.distplot(y_pred_train)\n",
    "    plt.title('Prediction vs Actual (Training)', fontsize=8)\n",
    "    plt.xlabel(target, fontsize=8)\n",
    "    plt.ylabel('Density', fontsize=8)\n",
    "    \n",
    "    plt.subplot(2,3,2)\n",
    "    sns.distplot((y_train - y_pred_train))\n",
    "    plt.title('Error Terms (Training)', fontsize=8)\n",
    "    plt.xlabel('Errors', fontsize=8)\n",
    "    plt.ylabel('Density', fontsize=8)\n",
    "    \n",
    "    plt.subplot(2,3,3)\n",
    "    plt.scatter(y_train, y_pred_train)\n",
    "    plt.plot([y_train.min(),y_train.max()],[y_train.min(),y_train.max()], 'r--')\n",
    "    plt.title('Train vs Prediction', fontsize=8)\n",
    "    plt.xlabel('Actual', fontsize=8)\n",
    "    plt.ylabel('Prediction', fontsize=8)\n",
    "    \n",
    "    plt.subplot(2,3,4)\n",
    "    sns.distplot(y_test)\n",
    "    sns.distplot(y_pred_test)\n",
    "    plt.title('Prediction vs Actual (Test)', fontsize=8)\n",
    "    plt.xlabel(target, fontsize=8)\n",
    "    plt.ylabel('Density', fontsize=8)\n",
    "    \n",
    "    plt.subplot(2,3,5)\n",
    "    sns.distplot((y_test - y_pred_test))\n",
    "    plt.title('Error Terms (Test)', fontsize=8)\n",
    "    plt.xlabel('Errors', fontsize=8)\n",
    "    plt.ylabel('Density', fontsize=8)\n",
    "    \n",
    "    plt.subplot(2,3,6)\n",
    "    plt.scatter(y_test, y_pred_test)\n",
    "    plt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()], 'r--')\n",
    "    plt.title('Test vs Prediction', fontsize=8)\n",
    "    plt.xlabel('Actual', fontsize=8)\n",
    "    plt.ylabel('Prediction', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ca28e-f330-4924-8049-5baa979388cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reg_preds(X, y_actual, y_pred, cols=[], first_count=None, rand_count=None):\n",
    "\n",
    "    if rand_count:\n",
    "        cols = random.sample(list(X.columns.values), rand_count)\n",
    "    \n",
    "    if first_count:\n",
    "        cols = X.columns.values[:first_count]\n",
    "    \n",
    "    num_rows = math.ceil(len(cols)/3)\n",
    "    \n",
    "    fig, ax = plt.figure(figsize=[15,3*num_rows])\n",
    "    for i, col in enumerate(cols):\n",
    "        plt.subplot(num_rows, 3, i+1)\n",
    "        plt.suptitle('Prediction vs Actual across Input Features', fontsize=16)\n",
    "\n",
    "        if X[col].dtype == 'float64':\n",
    "            sns.scatterplot(y=y_actual, x=X[col], label='Actual', alpha=1)\n",
    "            sns.scatterplot(y=y_pred, x=X[col], label='Prediction', alpha=0.5)\n",
    "\n",
    "        elif X[col].dtype == 'bool':\n",
    "            temp_actual = pd.DataFrame({'Label': 'Actual', 'Value': y_train, col: X_train[col]})\n",
    "            temp_pred = pd.DataFrame({'Label': 'Prediction', 'Value': y_pred, col: X_train[col]})\n",
    "            temp = pd.concat([temp_actual, temp_pred], ignore_index=True)\n",
    "            sns.boxplot(data=temp, x='Label', y='Value', hue=col)\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(target)\n",
    "\n",
    "        plt.title(col)\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a66b0-b21b-48bc-9307-d9a0dddcf5a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## f) Model Saving"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b79edb7f-8c32-4ab8-a52f-7fb16fdb778f",
   "metadata": {},
   "source": [
    "# save fitted classifier\n",
    "def pickle_model(clf, clf_name):\n",
    "    # save the model to disk\n",
    "    filename = 'Models/'+clf_name+'.sav'\n",
    "    pickle.dump(clf, open(filename, 'wb'))\n",
    "    \n",
    "    return \"Model saved as: \"+clf_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dfc3a5-1d34-44c1-9112-7016c3e03c3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4) XAI - Model Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c482930-1a3e-46ac-91df-109c2a213723",
   "metadata": {},
   "source": [
    "## a) SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3935debf-7b7d-4a43-be14-7f11dd421730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output various SHAP plots\n",
    "def run_shap(clf, X, i=0):\n",
    "    \n",
    "    #shap.initjs()\n",
    "    \n",
    "    explainer = shap.Explainer(clf, X)\n",
    "    shap_values = explainer(X)\n",
    "    \n",
    "    # force plot\n",
    "    #shap.force_plot(explainer.expected_value, shap_values[i], X[i], feature_names = features)\n",
    "    \n",
    "    # waterfall plot\n",
    "    plt.figure(figsize=(50, 20))\n",
    "    shap.plots.waterfall(shap_values[i], max_display=25, show=False)\n",
    "    plt.show()      \n",
    "    \n",
    "    # summary plot\n",
    "    plt.figure(figsize=(50, 20))\n",
    "    shap.summary_plot(shap_values, X, max_display=25, show=False)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # mean SHAP\n",
    "    plt.figure(figsize=(50, 20))\n",
    "    shap.plots.bar(shap_values, max_display=25, show=False)\n",
    "    plt.show()\n",
    "    \n",
    "    # dependence plot\n",
    "    #shap.dependence_plot(5, shap_values, X, feature_names=['annual_inc','fico'])\n",
    "    #shap.dependence_plot('annual_inc', shap_values[1], X, interaction_index=\"annual_inc\")\n",
    "    \n",
    "    return shap_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6497842-87d5-4cc4-8886-b0c9592f57ee",
   "metadata": {},
   "source": [
    "## b) LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "319eaeb5-59d2-4b7a-87bf-d2873ccf009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output LIME plot\n",
    "def run_lime(X_train, X_test, clf, i=0):\n",
    "        \n",
    "    # get the class names\n",
    "    class_names = ['No default', 'Default']\n",
    "\n",
    "    # get the (categorical) feature names\n",
    "    feature_names = list(X_train.columns)\n",
    "    \n",
    "    categorical_names = [col for col in X_train \n",
    "                if np.isin(X_train[col].dropna().unique(), [0, 1]).all()]\n",
    "    categorical_features = [X_train.columns.get_loc(c) for c in categorical_names \n",
    "                            if c in X_train]\n",
    "\n",
    "    # fit the Explainer on the train data\n",
    "    explainer = LimeTabularExplainer(X_train.values, \n",
    "                                     feature_names=feature_names, \n",
    "                                     class_names=class_names,\n",
    "                                     categorical_features=categorical_features, \n",
    "                                     categorical_names=categorical_names,\n",
    "                                     mode='classification')\n",
    "    \n",
    "    # local explanation on the i-th instance in test data\n",
    "    explaination = explainer.explain_instance(X_test.iloc[i], clf.predict_proba)\n",
    "    explaination.show_in_notebook(show_table = True, show_all = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796aa0dd-b2a7-4b0d-ae8c-1d3845b60527",
   "metadata": {
    "tags": []
   },
   "source": [
    "## c) PFI - Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6edf1bca-e5cc-4f33-b61b-18f51b042be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output PFI plot\n",
    "def run_pfi(X, y, clf):\n",
    "    \n",
    "    perm = PermutationImportance(clf, random_state=42).fit(X, y)\n",
    "    return eli5.show_weights(perm, feature_names=X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d79cfc-4722-4c4b-92a7-e407443f7c2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5) Master-Function (Full Pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86190716-7e29-4553-96cd-72a0bfef0e2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## a) Fit-Predict-Evaluate (single model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d36e251d-8f7c-4acc-a50e-c2c9d61e08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit classifier, get preds and probs, evaluate model\n",
    "def fit_pred_eval(X_test, y_test, X_train, y_train, clf, clf_name, thres=0.5, show=True):\n",
    "    \n",
    "    #start_time = time.time()\n",
    "    \n",
    "    # fit classifier\n",
    "    clf_fit = fit_model(X_train, y_train, clf)\n",
    "    \n",
    "    # evaluate classifier\n",
    "    model_eval, cm, roc = eval_model(X_test, y_test, X_train, y_train, clf_fit, clf_name, thres=thres, show=False)\n",
    "    \n",
    "    # save model\n",
    "    pickle_model(clf_fit, clf_name+'_'+datetime.now(pytz.timezone('US/Pacific')).strftime(\"%d-%m-%Y_%H-%M\"))\n",
    "    \n",
    "    \n",
    "    return model_eval.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a1641-f10f-473d-a04c-9252533cdf2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## b) End-to-End Pipeline (single/multiple models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2e06f1f-c083-40f8-8b41-29663ceb6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fun full ML pipeline on all combinations of selected algorithm types, years, sampling and scaling methods, output evaluation summary table\n",
    "def master_func_eval(clf_list, clf_name_list, sampling_methods, scaling_methods, \n",
    "                     target='default', thres=0.5, size=-1, show=False):\n",
    "    \n",
    "    # initialize summary tables for model evaluation\n",
    "    model_eval = pd.DataFrame(index=['RunTime', 'Accuracy', 'Bal_Acc', 'Precision', 'Recall', 'F1', 'AUC'])\n",
    "    \n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # data loading and splitting\n",
    "    X_train_main, X_test_main, y_train_main, y_test_main = load_and_split_data('Data/data_clean_full.csv', target, size=size, year=[yr])\n",
    "\n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    # set classifier index\n",
    "    i = 0\n",
    "\n",
    "    # loop through all committed classifiers, sampling and scaling methods\n",
    "    for clf in clf_list:   \n",
    "        for sampler in sampling_methods:   \n",
    "            for scaler in scaling_methods:\n",
    "\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # select datasets\n",
    "                X_train, y_train, X_test, y_test = select_X_y(X_train_main, y_train_main, X_test_main, y_test_main,\n",
    "                                                              sampler=sampler, scaler=scaler, size=size,\n",
    "                                                              sampling_strategy=1, version=0, n_neighbors=0)\n",
    "\n",
    "                # generate classifier name\n",
    "                clf_name = clf_name_list[i]\n",
    "                if sampler:\n",
    "                    clf_name += '_' + sampler\n",
    "\n",
    "                if scaler:\n",
    "                    clf_name += '_' + scaler\n",
    "\n",
    "                # fit, predict and evaluate model\n",
    "                model_eval[clf_name] = fit_pred_eval(X_train, y_train, X_test, y_test, clf, clf_name, thres=thres, show=show).iloc[:,1]\n",
    "\n",
    "                # store run time\n",
    "                model_eval.loc['RunTime', clf_name] = str(timedelta(seconds=time.time()-start_time+load_time))\n",
    "\n",
    "        # increase classifier index\n",
    "        i += 1\n",
    "\n",
    "    # transpose output table\n",
    "    model_eval = model_eval.transpose()\n",
    "    \n",
    "    # split clf_name\n",
    "    s = model_eval.index.str.split('_')\n",
    "    \n",
    "    # initialize new columns\n",
    "    model_eval[['Clf', 'Scaling', 'Sampling']] = ''\n",
    "    \n",
    "    # extract classifier, samping and scaling method\n",
    "    for i in range(model_eval.shape[0]):\n",
    "        model_eval['Clf'][i] = s[i][0]\n",
    "        if len(s[i])==4:\n",
    "            model_eval['Sampling'][i] = s[i][1]\n",
    "            model_eval['Scaling'][i] = s[i][2] \n",
    "        elif len(s[i])==3:\n",
    "            if s[i][2] in sampling_methods:\n",
    "                model_eval['Sampling'][i] = s[i][1]\n",
    "                model_eval['Scaling'][i] = 'non'\n",
    "            elif s[i][2] in scaling_methods:\n",
    "                model_eval['Scaling'][i] = s[i][1]\n",
    "                model_eval['Sampling'][i] = 'non'  \n",
    "        else:\n",
    "            model_eval['Sampling'][i] = 'non'\n",
    "            model_eval['Scaling'][i] = 'non'\n",
    "            \n",
    "    # save output table as csv\n",
    "    model_eval.to_csv('Master_output/master_output_'+datetime.now(pytz.timezone('US/Pacific')).strftime(\"%d-%m-%Y_%H-%M\")+'.csv')\n",
    "        \n",
    "    return model_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc64fdb4-c9de-40a6-8f22-731386ed17aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## c) Output Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b6f81d0-4627-4078-b376-bc9f95320102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance for all years and classifiers \n",
    "def plot_all_yrs(data):\n",
    "    \n",
    "    # drop RunTime column\n",
    "    df = data.drop(['RunTime'], inplace=False, axis=1)\n",
    "    \n",
    "    df = pd.melt(df, id_vars =['Clf','Sampling','Scaling'],\n",
    "                 value_vars =['Accuracy','Bal_Acc','Precision','Recall','F1','AUC'],\n",
    "                 var_name ='Metric', value_name ='Score')\n",
    "    \n",
    "    df_grouped = pd.DataFrame(df.groupby(['Metric'])['Score'].mean())\n",
    "    df_grouped.reset_index(inplace=True)\n",
    "    \n",
    "    # plot mean scores\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.lineplot(x = 'Year', y = 'Score', hue='Metric', data=df_grouped)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(x = 'Metric', y = 'Score', hue='Year', data=df_grouped)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "    plt.suptitle('Mean Scores by Year')\n",
    "    plt.show()\n",
    "    \n",
    "    df_grouped = pd.DataFrame(df.groupby(['Year','Clf','Metric'])['Score'].mean())\n",
    "    df_grouped.reset_index(inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    sns.catplot(x='Clf', y='Score',\n",
    "                col='Metric', kind='bar', col_wrap=6,\n",
    "                palette='deep', data=df_grouped[df_grouped['Year']==yr])\n",
    "    plt.suptitle('Year - %s' % (yr), y=1.05, size=24)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eb609c-547c-48df-a06f-572eb40c25e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
